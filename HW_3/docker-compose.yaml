version: "3.8"

services:
  namenode:
    build:
      context: hadoop/.
      target: build_hadoop
    hostname: namenode

# Сначала запусти меня
    #command: ["hdfs", "namenode", "-format", "-force"]
# А уже потом спокойно запускай меня
    command: ["hdfs", "namenode"]
    env_file:
      - ./hadoop.env
    ports:
      - 9870:9870
    volumes:
      - hadoop_namenode:/tmp/hadoop-root/
      - ./hadoop_conf/etc:/opt/hadoop/etc

  datanode1:
    build:
      context: hadoop/.
      target: build_hadoop
    hostname: datanode
    command: ["hdfs", "datanode"]
    env_file:
      - ./hadoop.env
    ports:
      - 9864:9864
    volumes:
      - hdfs1:/tmp/hadoop-hadoop
      - ./hadoop_conf/etc:/opt/hadoop/etc

  resourcemanager:
    build:
      context: hadoop/.
      target: build_hadoop
    hostname: resourcemanager
    command: ["yarn", "resourcemanager"]
    ports:
      - 8088:8088
    volumes:
      - resourcemanager:/tmp/hadoop-hadoop
      - ./hadoop_conf/etc:/opt/hadoop/etc

  nodemanager1:
    build:
      context: hadoop/.
      target: build_hadoop
    hostname: nodemanager1
    command: ["yarn", "nodemanager"]
    ports:
      - 19888:19888
      - 8042:8042
    expose:
      - 19888
      - 8042
    volumes:
      - nodemanager1:/tmp/hadoop-hadoop
      - ./hadoop_conf/etc:/opt/hadoop/etc

  postgres:
    image: postgres:10
    hostname: postgres
    environment:
      POSTGRES_DB: 'metastore_db'
      POSTGRES_USER: 'hive'
      POSTGRES_PASSWORD: 'password'
    ports:
      - '5432:5432'
    volumes:
      - hive-db:/var/lib/postgresql

  metastore:
    build:
      context: hadoop/.
      target: hive_hadoop
    hostname: metastore
    # Сначала запусти меня
    #command: ["schematool", "--dbType", "postgres", "--initSchema"]
    # А уже потом спокойно запускай меня
    command: [ "hive", "--service", "metastore" ]
    environment:
      - HIVE_SERVER2_THRIFT_PORT=10000
    ports:
      - 9083:9083
    expose:
      - 9083
    volumes:
      - ./hadoop_conf/etc:/opt/hadoop/etc
      - ./hive_conf:/opt/hive/conf

  hiveserver2:
    build:
      context: hadoop/.
      target: hive_hadoop
    environment:
      - HIVE_SERVER2_THRIFT_PORT=10000
    command: [ "hive", "--service", "hiveserver2" , "--hiveconf", "hive.root.logger=INFO,console"]
    ports:
      - 10000:10000
      - 10002:10002
    expose:
      - 10000
      - 10002
    volumes:
      - ./hadoop_conf/etc:/opt/hadoop/etc
      - ./hive_conf:/opt/hive/conf

  jupyter:
    build:
      context: hadoop/.
      target: jupyter_hadoop
    env_file:
      - ./hadoop.env
    ports:
      - 8888:8888
    volumes:
      - ./hadoop_conf/etc:/opt/hadoop/etc
      - ./notebooks:/opt/notebooks
    command: ["jupyter", "notebook", "--allow-root", "--ip", "0.0.0.0"]

  jobmanager:
    build: .
    image: pyflink/pyflink:1.16.0-scala_2.12
    volumes:
      - .:/opt/pyflink
    hostname: "jobmanager"
    expose:
      - "6123"
    ports:
      - "8081:8081"
    command: jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
  taskmanager:
    image: pyflink/pyflink:1.16.0-scala_2.12
    volumes:
    - .:/opt/pyflink
    expose:
      - "6121"
      - "6122"
    depends_on:
      - jobmanager
    command: taskmanager
    links:
      - jobmanager:jobmanager
    environment:
      - JOB_MANAGER_RPC_ADDRESS=jobmanager
  zookeeper:
    image: wurstmeister/zookeeper:3.4.6
    ulimits:
      nofile:
        soft: 65536
        hard: 65536
    ports:
      - "2181:2181"
  kafka:
    image: wurstmeister/kafka:2.13-2.8.1
    ports:
      - 29092:29092
    depends_on:
      - zookeeper
    environment:
      HOSTNAME_COMMAND: "route -n | awk '/UG[ \t]/{print $$2}'"
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: EXTERNAL_SAME_HOST://:29092,INTERNAL://:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka:9092,EXTERNAL_SAME_HOST://localhost:29092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL_SAME_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL

networks:
  default:
    name: yohoho

volumes:
  hive-db:
  hdfs1:
  resourcemanager:
  nodemanager1:
  hadoop_namenode:

